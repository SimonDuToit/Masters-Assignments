{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a329ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import squeezenet1_1, SqueezeNet1_1_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from torchmetrics import detection\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9266f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = squeezenet1_1(weights=SqueezeNet1_1_Weights.DEFAULT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ded94be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = torchvision.models.SqueezeNet1_1_Weights.DEFAULT.transforms()\n",
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a07a9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "nottestset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(5)\n",
    "\n",
    "trainset, validationset = torch.utils.data.random_split(nottestset, [0.8, 0.2], generator=generator1)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "validationloader = torch.utils.data.DataLoader(validationset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6bc104a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "SqueezeNet (SqueezeNet)                       [1, 3, 224, 224]     [1, 1000]            --                   True\n",
       "├─Sequential (features)                       [1, 3, 224, 224]     [1, 512, 13, 13]     --                   True\n",
       "│    └─Conv2d (0)                             [1, 3, 224, 224]     [1, 64, 111, 111]    1,792                True\n",
       "│    └─ReLU (1)                               [1, 64, 111, 111]    [1, 64, 111, 111]    --                   --\n",
       "│    └─MaxPool2d (2)                          [1, 64, 111, 111]    [1, 64, 55, 55]      --                   --\n",
       "│    └─Fire (3)                               [1, 64, 55, 55]      [1, 128, 55, 55]     --                   True\n",
       "│    │    └─Conv2d (squeeze)                  [1, 64, 55, 55]      [1, 16, 55, 55]      1,040                True\n",
       "│    │    └─ReLU (squeeze_activation)         [1, 16, 55, 55]      [1, 16, 55, 55]      --                   --\n",
       "│    │    └─Conv2d (expand1x1)                [1, 16, 55, 55]      [1, 64, 55, 55]      1,088                True\n",
       "│    │    └─ReLU (expand1x1_activation)       [1, 64, 55, 55]      [1, 64, 55, 55]      --                   --\n",
       "│    │    └─Conv2d (expand3x3)                [1, 16, 55, 55]      [1, 64, 55, 55]      9,280                True\n",
       "│    │    └─ReLU (expand3x3_activation)       [1, 64, 55, 55]      [1, 64, 55, 55]      --                   --\n",
       "│    └─Fire (4)                               [1, 128, 55, 55]     [1, 128, 55, 55]     --                   True\n",
       "│    │    └─Conv2d (squeeze)                  [1, 128, 55, 55]     [1, 16, 55, 55]      2,064                True\n",
       "│    │    └─ReLU (squeeze_activation)         [1, 16, 55, 55]      [1, 16, 55, 55]      --                   --\n",
       "│    │    └─Conv2d (expand1x1)                [1, 16, 55, 55]      [1, 64, 55, 55]      1,088                True\n",
       "│    │    └─ReLU (expand1x1_activation)       [1, 64, 55, 55]      [1, 64, 55, 55]      --                   --\n",
       "│    │    └─Conv2d (expand3x3)                [1, 16, 55, 55]      [1, 64, 55, 55]      9,280                True\n",
       "│    │    └─ReLU (expand3x3_activation)       [1, 64, 55, 55]      [1, 64, 55, 55]      --                   --\n",
       "│    └─MaxPool2d (5)                          [1, 128, 55, 55]     [1, 128, 27, 27]     --                   --\n",
       "│    └─Fire (6)                               [1, 128, 27, 27]     [1, 256, 27, 27]     --                   True\n",
       "│    │    └─Conv2d (squeeze)                  [1, 128, 27, 27]     [1, 32, 27, 27]      4,128                True\n",
       "│    │    └─ReLU (squeeze_activation)         [1, 32, 27, 27]      [1, 32, 27, 27]      --                   --\n",
       "│    │    └─Conv2d (expand1x1)                [1, 32, 27, 27]      [1, 128, 27, 27]     4,224                True\n",
       "│    │    └─ReLU (expand1x1_activation)       [1, 128, 27, 27]     [1, 128, 27, 27]     --                   --\n",
       "│    │    └─Conv2d (expand3x3)                [1, 32, 27, 27]      [1, 128, 27, 27]     36,992               True\n",
       "│    │    └─ReLU (expand3x3_activation)       [1, 128, 27, 27]     [1, 128, 27, 27]     --                   --\n",
       "│    └─Fire (7)                               [1, 256, 27, 27]     [1, 256, 27, 27]     --                   True\n",
       "│    │    └─Conv2d (squeeze)                  [1, 256, 27, 27]     [1, 32, 27, 27]      8,224                True\n",
       "│    │    └─ReLU (squeeze_activation)         [1, 32, 27, 27]      [1, 32, 27, 27]      --                   --\n",
       "│    │    └─Conv2d (expand1x1)                [1, 32, 27, 27]      [1, 128, 27, 27]     4,224                True\n",
       "│    │    └─ReLU (expand1x1_activation)       [1, 128, 27, 27]     [1, 128, 27, 27]     --                   --\n",
       "│    │    └─Conv2d (expand3x3)                [1, 32, 27, 27]      [1, 128, 27, 27]     36,992               True\n",
       "│    │    └─ReLU (expand3x3_activation)       [1, 128, 27, 27]     [1, 128, 27, 27]     --                   --\n",
       "│    └─MaxPool2d (8)                          [1, 256, 27, 27]     [1, 256, 13, 13]     --                   --\n",
       "│    └─Fire (9)                               [1, 256, 13, 13]     [1, 384, 13, 13]     --                   True\n",
       "│    │    └─Conv2d (squeeze)                  [1, 256, 13, 13]     [1, 48, 13, 13]      12,336               True\n",
       "│    │    └─ReLU (squeeze_activation)         [1, 48, 13, 13]      [1, 48, 13, 13]      --                   --\n",
       "│    │    └─Conv2d (expand1x1)                [1, 48, 13, 13]      [1, 192, 13, 13]     9,408                True\n",
       "│    │    └─ReLU (expand1x1_activation)       [1, 192, 13, 13]     [1, 192, 13, 13]     --                   --\n",
       "│    │    └─Conv2d (expand3x3)                [1, 48, 13, 13]      [1, 192, 13, 13]     83,136               True\n",
       "│    │    └─ReLU (expand3x3_activation)       [1, 192, 13, 13]     [1, 192, 13, 13]     --                   --\n",
       "│    └─Fire (10)                              [1, 384, 13, 13]     [1, 384, 13, 13]     --                   True\n",
       "│    │    └─Conv2d (squeeze)                  [1, 384, 13, 13]     [1, 48, 13, 13]      18,480               True\n",
       "│    │    └─ReLU (squeeze_activation)         [1, 48, 13, 13]      [1, 48, 13, 13]      --                   --\n",
       "│    │    └─Conv2d (expand1x1)                [1, 48, 13, 13]      [1, 192, 13, 13]     9,408                True\n",
       "│    │    └─ReLU (expand1x1_activation)       [1, 192, 13, 13]     [1, 192, 13, 13]     --                   --\n",
       "│    │    └─Conv2d (expand3x3)                [1, 48, 13, 13]      [1, 192, 13, 13]     83,136               True\n",
       "│    │    └─ReLU (expand3x3_activation)       [1, 192, 13, 13]     [1, 192, 13, 13]     --                   --\n",
       "│    └─Fire (11)                              [1, 384, 13, 13]     [1, 512, 13, 13]     --                   True\n",
       "│    │    └─Conv2d (squeeze)                  [1, 384, 13, 13]     [1, 64, 13, 13]      24,640               True\n",
       "│    │    └─ReLU (squeeze_activation)         [1, 64, 13, 13]      [1, 64, 13, 13]      --                   --\n",
       "│    │    └─Conv2d (expand1x1)                [1, 64, 13, 13]      [1, 256, 13, 13]     16,640               True\n",
       "│    │    └─ReLU (expand1x1_activation)       [1, 256, 13, 13]     [1, 256, 13, 13]     --                   --\n",
       "│    │    └─Conv2d (expand3x3)                [1, 64, 13, 13]      [1, 256, 13, 13]     147,712              True\n",
       "│    │    └─ReLU (expand3x3_activation)       [1, 256, 13, 13]     [1, 256, 13, 13]     --                   --\n",
       "│    └─Fire (12)                              [1, 512, 13, 13]     [1, 512, 13, 13]     --                   True\n",
       "│    │    └─Conv2d (squeeze)                  [1, 512, 13, 13]     [1, 64, 13, 13]      32,832               True\n",
       "│    │    └─ReLU (squeeze_activation)         [1, 64, 13, 13]      [1, 64, 13, 13]      --                   --\n",
       "│    │    └─Conv2d (expand1x1)                [1, 64, 13, 13]      [1, 256, 13, 13]     16,640               True\n",
       "│    │    └─ReLU (expand1x1_activation)       [1, 256, 13, 13]     [1, 256, 13, 13]     --                   --\n",
       "│    │    └─Conv2d (expand3x3)                [1, 64, 13, 13]      [1, 256, 13, 13]     147,712              True\n",
       "│    │    └─ReLU (expand3x3_activation)       [1, 256, 13, 13]     [1, 256, 13, 13]     --                   --\n",
       "├─Sequential (classifier)                     [1, 512, 13, 13]     [1, 1000, 1, 1]      --                   True\n",
       "│    └─Dropout (0)                            [1, 512, 13, 13]     [1, 512, 13, 13]     --                   --\n",
       "│    └─Conv2d (1)                             [1, 512, 13, 13]     [1, 1000, 13, 13]    513,000              True\n",
       "│    └─ReLU (2)                               [1, 1000, 13, 13]    [1, 1000, 13, 13]    --                   --\n",
       "│    └─AdaptiveAvgPool2d (3)                  [1, 1000, 13, 13]    [1, 1000, 1, 1]      --                   --\n",
       "=============================================================================================================================\n",
       "Total params: 1,235,496\n",
       "Trainable params: 1,235,496\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 351.74\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 20.71\n",
       "Params size (MB): 4.94\n",
       "Estimated Total Size (MB): 26.26\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, \n",
    "        input_size=(1, 3, 224, 224),\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d9f6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = torch.nn.Sequential(\n",
    "                        torch.nn.AdaptiveAvgPool2d(1),\n",
    "                        torch.nn.Flatten(),\n",
    "                        torch.nn.Linear(in_features=512, \n",
    "                            out_features=10,\n",
    "                            bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "803da5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "209f4254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 4.287\n",
      "[1,  4000] loss: 4.148\n",
      "[1,  6000] loss: 3.739\n",
      "[1,  8000] loss: 3.734\n",
      "[1, 10000] loss: 3.452\n",
      "[2,  2000] loss: 2.940\n",
      "[2,  4000] loss: 3.393\n",
      "[2,  6000] loss: 3.409\n",
      "[2,  8000] loss: 3.515\n",
      "[2, 10000] loss: 3.354\n",
      "Finished Training\n",
      "1814.7992973327637\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "start = time()\n",
    "for epoch in range(2):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85864b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 75 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test set: {100 * correct // total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecd64afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.features[9:].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84190444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.972\n",
      "[1,  4000] loss: 0.587\n",
      "[1,  6000] loss: 0.518\n",
      "[1,  8000] loss: 0.483\n",
      "[1, 10000] loss: 0.453\n",
      "Finished Training\n",
      "1135.1543862819672\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "start = time()\n",
    "for epoch in range(1):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d17a033e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 84 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test set: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0d84a00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2acc6fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.248\n",
      "[1,  4000] loss: 0.253\n",
      "[1,  6000] loss: 0.251\n",
      "[1,  8000] loss: 0.254\n",
      "[1, 10000] loss: 0.246\n",
      "Finished Training\n",
      "1913.9227967262268\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "start = time()\n",
    "for epoch in range(1):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4ecdf1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 88 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test set: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010916b",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd788e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd2a4f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Flatten(start_dim=1, end_dim=-1)\n",
       "      (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights = weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06354bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'validation' to 'C:\\Users\\simon\\fiftyone\\coco-2017\\validation' if necessary\n",
      "Found annotations at 'C:\\Users\\simon\\fiftyone\\coco-2017\\raw\\instances_val2017.json'\n",
      "Sufficient images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading existing dataset 'coco-2017-validation-100'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "fiftyone_data = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"validation\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=[\"cat\", \"dog\"],\n",
    "    max_samples=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf595d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.coco as fouc\n",
    "\n",
    "#adapted from https://github.com/voxel51/fiftyone-examples/blob/master/examples/pytorch_detection_training.ipynb\n",
    "class FiftyOneTorchDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A class to construct a PyTorch dataset from a FiftyOne dataset.\n",
    "    \n",
    "    Args:\n",
    "        fiftyone_dataset: a FiftyOne dataset or view that will be used for training or testing\n",
    "        transforms (None): a list of PyTorch transforms to apply to images and targets when loading\n",
    "        gt_field (\"ground_truth\"): the name of the field in fiftyone_dataset that contains the \n",
    "            desired labels to load\n",
    "        classes (None): a list of class strings that are used to define the mapping between\n",
    "            class names and indices. If None, it will use all classes present in the given fiftyone_dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        fiftyone_dataset,\n",
    "        transforms=None,\n",
    "        gt_field=\"ground_truth\",\n",
    "        classes=None,\n",
    "    ):\n",
    "        self.samples = fiftyone_dataset\n",
    "        self.transforms = transforms\n",
    "        self.gt_field = gt_field\n",
    "\n",
    "        self.img_paths = self.samples.values(\"filepath\")\n",
    "\n",
    "        self.classes = classes\n",
    "        if not self.classes:\n",
    "            self.classes = self.samples.distinct(\n",
    "                \"%s.detections.label\" % gt_field\n",
    "            )\n",
    "\n",
    "        if self.classes[0] != \"background\":\n",
    "            self.classes = [\"background\"] + self.classes\n",
    "\n",
    "        self.labels_map_rev = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        sample = self.samples[img_path]\n",
    "        metadata = sample.metadata\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        area = []\n",
    "        iscrowd = []\n",
    "        detections = sample[self.gt_field].detections\n",
    "        for det in detections:\n",
    "            category_id = self.labels_map_rev[det.label]\n",
    "            coco_obj = fouc.COCOObject.from_label(\n",
    "                det, metadata, category_id=category_id,\n",
    "            )\n",
    "            x, y, w, h = coco_obj.bbox\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(coco_obj.category_id)\n",
    "            area.append(coco_obj.area)\n",
    "            iscrowd.append(coco_obj.iscrowd)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.as_tensor([idx])\n",
    "        target[\"area\"] = torch.as_tensor(area, dtype=torch.float32)\n",
    "        target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def get_classes(self):\n",
    "        return self.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29f31877",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FiftyOneTorchDataset(fiftyone_data)\n",
    "detection_transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58ebcc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map': tensor(0.5207),\n",
       " 'map_50': tensor(0.6882),\n",
       " 'map_75': tensor(0.6310),\n",
       " 'map_small': tensor(-1.),\n",
       " 'map_medium': tensor(0.3168),\n",
       " 'map_large': tensor(0.5461),\n",
       " 'mar_1': tensor(0.4907),\n",
       " 'mar_10': tensor(0.6566),\n",
       " 'mar_100': tensor(0.6566),\n",
       " 'mar_small': tensor(-1.),\n",
       " 'mar_medium': tensor(0.3167),\n",
       " 'mar_large': tensor(0.6876),\n",
       " 'map_per_class': tensor(-1.),\n",
       " 'mar_100_per_class': tensor(-1.)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "targets = []\n",
    "i = 0\n",
    "\n",
    "sorter = np.argsort(dataset.classes)\n",
    "for data in dataset:\n",
    "    image = data[0]\n",
    "    target = data[1]   \n",
    "    prediction = model(detection_transform(image)[None,:,:,:])[0]\n",
    "    \n",
    "    label_strings = np.asarray(weights.meta[\"categories\"])[prediction['labels']]\n",
    "    converted_labels = torch.tensor(sorter[np.searchsorted(dataset.classes, label_strings, sorter=sorter)])\n",
    "    prediction['labels'] = converted_labels\n",
    "    \n",
    "    predictions += [prediction]\n",
    "    targets += [target]\n",
    "    \n",
    "    i += 1\n",
    "    if (i % 10 == 0):\n",
    "        print(i)\n",
    "        \n",
    "\n",
    "metric = detection.MeanAveragePrecision()\n",
    "metric.update(predictions, targets)\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e3dcbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_colors(n):\n",
    "    colors = [()]*n\n",
    "    for i in range(n):\n",
    "        random_color = tuple(np.random.choice(range(255),size=3))\n",
    "        colors[i] = random_color\n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ace5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL_transform = transforms.Compose([transforms.PILToTensor()])\n",
    "random_colors = make_random_colors(len(weights.meta[\"categories\"]))\n",
    "\n",
    "def show_prediction(img, prediction):\n",
    "    img = PIL_transform(img)\n",
    "    cutoff = (prediction['scores'] > 0.8).sum()\n",
    "    labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "    box = draw_bounding_boxes(img, boxes=prediction[\"boxes\"][:cutoff],\n",
    "                              labels=labels[:cutoff],\n",
    "                              colors=[random_colors[l] for l in prediction[\"labels\"]],\n",
    "                              width=4, font_size=30)\n",
    "    im = to_pil_image(box.detach())\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6235de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    prediction = model(detection_transform(dataset[i][0])[None,:,:,:])[0]\n",
    "    show_prediction(dataset[i][0], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739db15b",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af65d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note - Q3 code was executed in collab due to high memory requirements, so cell output is not visible here\n",
    "from zipfile import ZipFile\n",
    "#!unzip lfwcrop_color.zip\n",
    "with ZipFile(\"lfwcrop_color.zip\", 'r') as zObject:\n",
    "  \n",
    "    # Extracting all the members of the zip \n",
    "    # into a specific location.\n",
    "    zObject.extractall(\n",
    "        path=\"lfw_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e57da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#structure target data\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "d = 0\n",
    "for filename in glob.glob('lfw_data/lfwcrop_color/faces/*ppm'):\n",
    "    img = Image.open(filename)\n",
    "\n",
    "    name = filename[filename.index('faces/')+6:filename.index('.')]\n",
    "\n",
    "    newfilename = \"lfw_data_structured/target/\" + name + \"/\" + name + \".jpg\"\n",
    "    os.makedirs(os.path.dirname(newfilename), exist_ok=True)\n",
    "    img = img.save(newfilename)\n",
    "    d+=1\n",
    "    if (d % 1000 == 0):\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ee134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create (noisy) input data\n",
    "d = 0\n",
    "for filename in glob.glob('lfw_data/lfwcrop_color/faces/*ppm'):\n",
    "    img = Image.open(filename)\n",
    "    \n",
    "    noise = np.random.normal(0.0, 255.0 * 0.1, (10, 64, 64, 3))\n",
    "    noisy_images = np.clip((noise + np.array(img)).astype(np.uint8), 0, 255)\n",
    "    \n",
    "    name = filename[filename.index('faces/')+6:filename.index('.')]\n",
    "    i = 1\n",
    "    for noisy_image in noisy_images:\n",
    "        new_img = Image.fromarray(noisy_image)\n",
    "        newfilename = \"lfw_data_structured/noisy/\" + name + \"/\" + name + \"_%d.jpg\"%i\n",
    "        os.makedirs(os.path.dirname(newfilename), exist_ok=True)\n",
    "        new_img = new_img.save(newfilename)\n",
    "        i+=1\n",
    "    d+=1\n",
    "    if (d % 1000 == 0):\n",
    "        print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5b45b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     ])\n",
    "dataset_noise = torchvision.datasets.ImageFolder('lfw_data_structured/noisy', transform=transform)\n",
    "dataset_target = torchvision.datasets.ImageFolder('lfw_data_structured/target', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "12533a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, noisy, target):\n",
    "        self.noisy = noisy\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image, target_image = self.noisy[index][0], self.target[self.noisy[index][1]][0]\n",
    "        return noisy_image, target_image\n",
    "    \n",
    "dataset = DualImageDataset(dataset_noise, dataset_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1a4ace70",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(5)\n",
    "\n",
    "trainset, validationset, testset = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1], generator=generator1)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "validationloader = torch.utils.data.DataLoader(validationset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "07defdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#denoising neural network implementation\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2, return_indices=True)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)    \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) \n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(2, 2)\n",
    "        self.conv5 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(64, 32, 3, padding=1) \n",
    "        self.conv8 = nn.Conv2d(32, 3, 3, padding=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x, i1 = self.pool(F.relu(self.conv1(x)))      \n",
    "        x, i2 = self.pool(F.relu(self.conv2(x)))    \n",
    "        x, i3 = self.pool(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(self.unpool(x, i3)))\n",
    "        x = F.relu(self.conv7(self.unpool(x, i2)))\n",
    "        x = F.relu(self.conv8(self.unpool(x, i1)))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = AutoEncoder()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bcecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 1000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ce4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_at = 5\n",
    "k = int(np.floor(sample_at/batch_size))\n",
    "\n",
    "my_sample = next(itertools.islice(trainloader, k, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b07dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy, target = my_sample[0], my_sample[1]\n",
    "plt.imshow(target[1].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162203f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noisy[1].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net = net.to('cpu')\n",
    "    denoised = net(noisy)\n",
    "    plt.imshow(denoised[1].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71916bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "net = net.to(device)\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        i += 1\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total += loss\n",
    "\n",
    "print(f'Average MSE on the test set: {total / i}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
