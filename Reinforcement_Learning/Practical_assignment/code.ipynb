{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ff33772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.0 (SDL 2.28.0, Python 3.9.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import minigrid\n",
    "import random\n",
    "from minigrid.wrappers import *\n",
    "import time\n",
    "import pickle\n",
    "from os.path import exists\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae1fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess observation state\n",
    "def preprocess(observation):\n",
    "    (rows, cols, x) = obs.shape\n",
    "    tmp = np.reshape(obs,[rows*cols*x,1], 'F')[0:rows*cols]\n",
    "    return np.reshape(tmp, [rows,cols],'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877e4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state hash function\n",
    "def hashState(state):\n",
    "    hash = state.tobytes()\n",
    "    return hash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca05c6",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c2e5179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training iteration 10\n",
      "Done training iteration 20\n",
      "Best average reward is 0.9219403645833336 achieved with hyperparmater configuration: [0.56, 0.12, 0.89, 0.67]\n"
     ]
    }
   ],
   "source": [
    "# Make the gym environment\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "env = ImgObsWrapper(env)\n",
    "max_steps = env.max_steps\n",
    "numActions = 2\n",
    "N = 30\n",
    "best_result = 0\n",
    "\n",
    "# Hyperparameter tuning\n",
    "for n in range(N):\n",
    "    \n",
    "    # declare the variable to store the tabular value-function\n",
    "    Q = {}\n",
    "    \n",
    "    # set hyperparameters  \n",
    "    episodes = 3000\n",
    "    epsilon_max = np.round(np.random.rand()/2 + 0.5, 2)\n",
    "    epsilon_min = np.round(np.random.rand()/2, 2)\n",
    "    alpha = np.round(np.random.rand(), 2)\n",
    "    discount = np.round(np.random.rand(), 2) \n",
    "    total_reward = 0\n",
    "\n",
    "    epsilon = epsilon_max\n",
    "    for e in range(episodes):\n",
    "        # reset the environment\n",
    "        obs, _ = env.reset()\n",
    "        currentS = preprocess(obs)   \n",
    "        for i in range(0, max_steps):\n",
    "\n",
    "            # hash state\n",
    "            currentS_Key = hashState(currentS)\n",
    "\n",
    "            if currentS_Key not in Q:\n",
    "                Q[currentS_Key] = np.zeros(3)\n",
    "\n",
    "            # Choose an action\n",
    "            # perform epsilon greedy action\n",
    "            if (random.random() < epsilon):\n",
    "                # Explore the environment by selecting a random action\n",
    "                a = random.randint(0, numActions)\n",
    "            else:\n",
    "                # Exploit the environment by selecting an action that is the maximum of the value function at the current State\n",
    "                a = np.argmax(Q[currentS_Key])\n",
    "\n",
    "            # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "            # 'done' indicate if the termination state was reached\n",
    "            obs, reward, done, truncated, info = env.step(a)\n",
    "\n",
    "            # extract the next state from the observation and hash it\n",
    "            nextS = preprocess(obs)\n",
    "            nextS_Key = hashState(nextS)\n",
    "            if nextS_Key not in Q:\n",
    "                Q[nextS_Key] = np.zeros(3)\n",
    "\n",
    "            # select next action greedily\n",
    "            nextA = np.argmax(Q[nextS_Key])\n",
    "\n",
    "            # update value in Q-table\n",
    "            Q[currentS_Key][a] = Q[currentS_Key][a] + alpha*(reward+discount*Q[nextS_Key][nextA]-Q[currentS_Key][a])\n",
    "\n",
    "            if (done == True or truncated == True):\n",
    "                # if agent reached its goal successfully\n",
    "                steps_done = i\n",
    "                break\n",
    "\n",
    "            # since the episode is not done, store the next state as the current state for the next step\n",
    "            currentS = nextS\n",
    "            \n",
    "        total_reward += reward\n",
    "\n",
    "        # anneal epsilon\n",
    "        epsilon = max(epsilon*0.999, epsilon_min)\n",
    "        \n",
    "    if (n % 10 == 0 and n > 0):\n",
    "        print('Done training iteration', n)\n",
    "        \n",
    "    if total_reward/episodes > best_result:\n",
    "            best_result = total_reward/episodes\n",
    "            best_params = [epsilon_max, epsilon_min, alpha, discount]\n",
    "            best_Q = Q.copy()\n",
    "    \n",
    "env.close()\n",
    "\n",
    "print('Best average reward is', best_result, 'achieved with hyperparmater configuration:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db528c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Done training...\n"
     ]
    }
   ],
   "source": [
    "# Train with best hypermarameters\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# declare the variable to store the tabular value-function\n",
    "Q = {}\n",
    "numActions = 2\n",
    "episodes = 3000\n",
    "epsilon_max = 0.56\n",
    "epsilon_min = 0.12\n",
    "alpha = 0.89\n",
    "discount = 0.67\n",
    "max_steps = env.max_steps\n",
    "\n",
    "# Use a wrapper so the observation only contains the grid information\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "print('Start training...')\n",
    "epsilon = epsilon_max\n",
    "total_reward = 0\n",
    "for e in range(episodes):\n",
    "    # reset the environment\n",
    "    obs, _ = env.reset()\n",
    "    currentS = preprocess(obs)   \n",
    "    for i in range(0, max_steps):\n",
    "\n",
    "        # hash state\n",
    "        currentS_Key = hashState(currentS)\n",
    "        \n",
    "        if currentS_Key not in Q:\n",
    "            Q[currentS_Key] = np.zeros(3)\n",
    "        \n",
    "        # Choose an action\n",
    "        # perform epsilon greedy action\n",
    "        if (random.random() < epsilon):\n",
    "            # Explore the environment by selecting a random action\n",
    "            a = random.randint(0, numActions)\n",
    "        else:\n",
    "            # Exploit the environment by selecting an action that is the maximum of the value function at the current State\n",
    "            a = np.argmax(Q[currentS_Key])\n",
    "\n",
    "        # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "        # 'done' indicate if the termination state was reached\n",
    "        obs, reward, done, truncated, info = env.step(a)\n",
    "        \n",
    "        # extract the next state from the observation and hash it\n",
    "        nextS = preprocess(obs)\n",
    "        nextS_Key = hashState(nextS)\n",
    "        if nextS_Key not in Q:\n",
    "            Q[nextS_Key] = np.zeros(3)\n",
    "        \n",
    "        # select next action greedily\n",
    "        nextA = np.argmax(Q[nextS_Key])\n",
    "        \n",
    "        # update value in Q-table\n",
    "        Q[currentS_Key][a] = Q[currentS_Key][a] + alpha*(reward+discount*Q[nextS_Key][nextA]-Q[currentS_Key][a])\n",
    "         \n",
    "        if (done == True or truncated == True):\n",
    "            # if agent reached its goal successfully\n",
    "            steps_done = i\n",
    "            break\n",
    "            \n",
    "        # since the episode is not done, store the next state as the current state for the next step\n",
    "        currentS = nextS\n",
    "    total_reward += reward\n",
    "    writer.add_scalar(\"Reward/training\", total_reward/(e+1), e)\n",
    "    \n",
    "    # anneal epsilon\n",
    "    epsilon = max(epsilon*0.999, epsilon_min)\n",
    "        \n",
    "print('Done training...')\n",
    "env.close()\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "filename = 'q-learn_qtable.pickle'\n",
    "\n",
    "# Saving the value-function to file\n",
    "with open(filename, 'wb') as handle:\n",
    "    pickle.dump(Q, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10006d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9f67573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing Q values\n"
     ]
    }
   ],
   "source": [
    "# Loading the value-function from file\n",
    "filename = 'q-learn_qtable.pickle'\n",
    "if (exists(filename)):\n",
    "    print('Loading existing Q values')\n",
    "    # Load data (deserialize)\n",
    "    with open(filename, 'rb') as handle:\n",
    "        Q = pickle.load(handle)\n",
    "        handle.close()\n",
    "else:\n",
    "    print('Filename %s does not exist, could not load data' % filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9693d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode successfully taking 12 steps and receiving reward 0.957812\n"
     ]
    }
   ],
   "source": [
    "# Visualize policy\n",
    "\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0', render_mode='human')\n",
    "env = ImgObsWrapper(env)\n",
    "# reset the environment\n",
    "obs, _ = env.reset()\n",
    "currentS = preprocess(obs)\n",
    "for i in range(0, max_steps):\n",
    "    \n",
    "    currentS_Key = hashState(currentS)\n",
    "\n",
    "    if currentS_Key not in Q:\n",
    "        Q[currentS_Key] = np.zeros(3)\n",
    "\n",
    "    # Choose an action\n",
    "    a = np.argmax(Q[currentS_Key])\n",
    "\n",
    "    # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "    # 'done' indicate if the termination state was reached\n",
    "    obs, reward, done, truncated, info = env.step(a)\n",
    "    # extract the next state from the observation\n",
    "    nextS = preprocess(obs)\n",
    "\n",
    "    if (done == True):\n",
    "        # if agent reached its goal successfully\n",
    "        print('Finished episode successfully taking %d steps and receiving reward %f' % (i+1, reward))\n",
    "        break\n",
    "    if (truncated == True):\n",
    "        # agent failed to reach its goal successfully \n",
    "        print('Truncated episode taking %d steps and receiving reward %f' % (i+1, reward))\n",
    "        break\n",
    "\n",
    "    # since the episode is not done, store the next state as the current state for the next step\n",
    "    currentS = nextS\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75e60cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion rate is 1.0\n",
      "Average number of steps is 12.0\n",
      "Average reward over 1000 runs is 0.9578\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model for 1000 iterations\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "env = ImgObsWrapper(env)\n",
    "mean_reward = 0\n",
    "completion_rate = 0\n",
    "average_steps = 0\n",
    "\n",
    "for run in range(1000):\n",
    "    # reset the environment\n",
    "    obs, _ = env.reset()\n",
    "    currentS = preprocess(obs)\n",
    "\n",
    "    for i in range(0, max_steps):\n",
    "\n",
    "        # hash state\n",
    "        currentS_Key = hashState(currentS)\n",
    "\n",
    "        if currentS_Key not in Q:\n",
    "            Q[currentS_Key] = np.zeros(3)\n",
    "\n",
    "        # Choose an action\n",
    "        a = np.argmax(Q[currentS_Key])\n",
    "\n",
    "        # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "        # 'done' indicate if the termination state was reached\n",
    "        obs, reward, done, truncated, info = env.step(a)\n",
    "        \n",
    "        # extract the next state from the observation\n",
    "        nextS = preprocess(obs)\n",
    "\n",
    "        if (done == True):\n",
    "            # if agent reached its goal successfully\n",
    "            mean_reward += reward\n",
    "            completion_rate += 1\n",
    "            average_steps += i+1\n",
    "            break\n",
    "        if (truncated == True):\n",
    "            # agent failed to reach its goal successfully \n",
    "            mean_reward += reward\n",
    "            average_steps += i+1\n",
    "            break\n",
    "\n",
    "        # since the episode is not done, store the next state as the current state for the next step\n",
    "        currentS = nextS\n",
    "    \n",
    "env.close()\n",
    "\n",
    "completion_rate /= 1000\n",
    "average_steps /= 1000\n",
    "print('Completion rate is', completion_rate)\n",
    "print('Average number of steps is', average_steps)\n",
    "print('Average reward over 1000 runs is', np.round(mean_reward/1000, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88963524",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0362690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training iteration 10\n",
      "Done training iteration 20\n",
      "Best average reward is 0.9108329427083401 achieved with hyperparamater configuration: [0.61, 0.03, 0.32, 0.44]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "env = ImgObsWrapper(env)\n",
    "max_steps = env.max_steps\n",
    "numActions = 2\n",
    "N = 30\n",
    "best_result = 0\n",
    "for n in range(N):\n",
    "    \n",
    "    # declare the variable to store the tabular value-function\n",
    "    Q = {}\n",
    "    \n",
    "    # set hyperparameters  \n",
    "    episodes = 3000\n",
    "    epsilon_max = np.round(np.random.rand()/2 + 0.5, 2)\n",
    "    epsilon_min = np.round(np.random.rand()/2, 2)\n",
    "    alpha = np.round(np.random.rand(), 2)\n",
    "    discount = np.round(np.random.rand(), 2) \n",
    "    total_reward = 0\n",
    "\n",
    "    epsilon = epsilon_max\n",
    "    for e in range(episodes):\n",
    "        # reset the environment\n",
    "        obs, _ = env.reset()\n",
    "        currentS = preprocess(obs)   \n",
    "        a = random.randint(0, numActions)\n",
    "        for i in range(0, max_steps):\n",
    "\n",
    "            # hash state\n",
    "            currentS_Key = hashState(currentS)\n",
    "\n",
    "            if currentS_Key not in Q:\n",
    "                Q[currentS_Key] = np.zeros(3)\n",
    "\n",
    "            # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "            # 'done' indicate if the termination state was reached\n",
    "            obs, reward, done, truncated, info = env.step(a)\n",
    "\n",
    "            # extract the next state from the observation and hash it\n",
    "            nextS = preprocess(obs)\n",
    "            nextS_Key = hashState(nextS)\n",
    "            if nextS_Key not in Q:\n",
    "                Q[nextS_Key] = np.zeros(3)\n",
    "\n",
    "            # select next action epsilon-greedily\n",
    "            if (random.random() < epsilon):\n",
    "                # Explore the environment by selecting a random action\n",
    "                nextA = random.randint(0, numActions)\n",
    "            else:\n",
    "                # Exploit the environment by selecting an action that is the maximum of the value function at the current State\n",
    "                nextA = np.argmax(Q[nextS_Key])\n",
    "\n",
    "            # Update values in Q-table\n",
    "            Q[currentS_Key][a] = Q[currentS_Key][a] + alpha*(reward+discount*Q[nextS_Key][nextA]-Q[currentS_Key][a])\n",
    "\n",
    "            if (done == True or truncated == True):\n",
    "                # if agent reached its goal successfully\n",
    "                steps_done = i\n",
    "                break\n",
    "\n",
    "            # store the next state and action as the current state and action for the next step\n",
    "            currentS = nextS\n",
    "            a = nextA\n",
    "            \n",
    "        total_reward += reward\n",
    "\n",
    "        # anneal epsilon\n",
    "        epsilon = max(epsilon*0.999, epsilon_min)\n",
    "        \n",
    "    if (n % 10 == 0 and n > 0):\n",
    "        print('Done training iteration', n)\n",
    "        \n",
    "    if total_reward/episodes > best_result:\n",
    "            best_result = total_reward/episodes\n",
    "            best_params = [epsilon_max, epsilon_min, alpha, discount]\n",
    "            best_Q = Q.copy()\n",
    "    \n",
    "env.close()\n",
    "\n",
    "print('Best average reward is', best_result, 'achieved with hyperparamater configuration:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c3ee39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Done training...\n"
     ]
    }
   ],
   "source": [
    "# Train with best hypermarameters\n",
    "\n",
    "# Make the gym environment\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# declare the variable to store the tabular value-function\n",
    "Q = {}\n",
    "numActions = 2\n",
    "episodes = 3000\n",
    "epsilon_max = 0.61\n",
    "epsilon_min = 0.03\n",
    "alpha = 0.32\n",
    "discount = 0.44\n",
    "max_steps = env.max_steps\n",
    "\n",
    "# Use a wrapper so the observation only contains the grid information\n",
    "env = ImgObsWrapper(env)\n",
    "\n",
    "print('Start training...')\n",
    "epsilon = epsilon_max\n",
    "total_reward = 0\n",
    "for e in range(episodes):\n",
    "    # reset the environment\n",
    "    obs, _ = env.reset()\n",
    "    currentS = preprocess(obs)   \n",
    "    a = random.randint(0, numActions)\n",
    "    for i in range(0, max_steps):\n",
    "\n",
    "        # hash state\n",
    "        currentS_Key = hashState(currentS)\n",
    "\n",
    "        if currentS_Key not in Q:\n",
    "            Q[currentS_Key] = np.zeros(3)\n",
    "\n",
    "        # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "        # 'done' indicate if the termination state was reached\n",
    "        obs, reward, done, truncated, info = env.step(a)\n",
    "\n",
    "        # extract the next state from the observation and hash it\n",
    "        nextS = preprocess(obs)\n",
    "        nextS_Key = hashState(nextS)\n",
    "        if nextS_Key not in Q:\n",
    "            Q[nextS_Key] = np.zeros(3)\n",
    "\n",
    "        # select next action epsilon-greedily\n",
    "        if (random.random() < epsilon):\n",
    "            # Explore the environment by selecting a random action\n",
    "            nextA = random.randint(0, numActions)\n",
    "        else:\n",
    "            # Exploit the environment by selecting an action that is the maximum of the value function at the current State\n",
    "            nextA = np.argmax(Q[nextS_Key])\n",
    "\n",
    "        # Update values in Q-table\n",
    "        Q[currentS_Key][a] = Q[currentS_Key][a] + alpha*(reward+discount*Q[nextS_Key][nextA]-Q[currentS_Key][a])\n",
    "\n",
    "        if (done == True or truncated == True):\n",
    "            # if agent reached its goal successfully\n",
    "            steps_done = i\n",
    "            break\n",
    "\n",
    "        # store the next state and action as the current state and action for the next step\n",
    "        currentS = nextS\n",
    "        a = nextA\n",
    "    total_reward += reward\n",
    "    writer.add_scalar(\"Reward/training\", total_reward/(e+1), e)\n",
    "    \n",
    "    # anneal epsilon\n",
    "    epsilon = max(epsilon*0.999, epsilon_min)\n",
    "        \n",
    "print('Done training...')\n",
    "env.close()\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "filename = 'SARSA_qtable.pickle'\n",
    "\n",
    "# Saving the value-function to file\n",
    "with open(filename, 'wb') as handle:\n",
    "    pickle.dump(Q, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4249cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing Q values\n"
     ]
    }
   ],
   "source": [
    "# Loading the value-function from file\n",
    "filename = 'SARSA_qtable.pickle'\n",
    "if (exists(filename)):\n",
    "    print('Loading existing Q values')\n",
    "    # Load data (deserialize)\n",
    "    with open(filename, 'rb') as handle:\n",
    "        Q = pickle.load(handle)\n",
    "        handle.close()\n",
    "else:\n",
    "    print('Filename %s does not exist, could not load data' % filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d169a53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode successfully taking 12 steps and receiving reward 0.957812\n"
     ]
    }
   ],
   "source": [
    "# Visualize optimal policy\n",
    "\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0', render_mode='human')\n",
    "env = ImgObsWrapper(env)\n",
    "max_steps = env.max_steps\n",
    "# reset the environment\n",
    "obs, _ = env.reset()\n",
    "time.sleep(2)\n",
    "currentS = preprocess(obs)\n",
    "for i in range(0, max_steps):\n",
    "    currentS_Key = hashState(currentS)\n",
    "\n",
    "    if currentS_Key not in Q:\n",
    "        Q[currentS_Key] = np.zeros(3)\n",
    "\n",
    "    # Choose an action\n",
    "    a = np.argmax(Q[currentS_Key])\n",
    "\n",
    "    # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "    # 'done' indicate if the termination state was reached\n",
    "    obs, reward, done, truncated, info = env.step(a)\n",
    "    # extract the next state from the observation\n",
    "    nextS = preprocess(obs)\n",
    "\n",
    "    #env.render() # render the environment, this does not work inside Jupyter notebook\n",
    "\n",
    "    # sleep for 50 milliseconds so we can see the rendering of the environment. \n",
    "    #time.sleep(0.05) # When training without rendering remove this line\n",
    "    if (done == True):\n",
    "        # if agent reached its goal successfully\n",
    "        print('Finished episode successfully taking %d steps and receiving reward %f' % (i+1, reward))\n",
    "        break\n",
    "    if (truncated == True):\n",
    "        # agent failed to reach its goal successfully \n",
    "        print('Truncated episode taking %d steps and receiving reward %f' % (i+1, reward))\n",
    "        break\n",
    "\n",
    "    # since the episode is not done, store the next state as the current state for the next step\n",
    "    currentS = nextS\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f2939ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion rate is 1.0\n",
      "Average number of steps is 12.0\n",
      "Average reward over 1000 runs is 0.9578\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model for 1000 iterations\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "env = ImgObsWrapper(env)\n",
    "mean_reward = 0\n",
    "completion_rate = 0\n",
    "average_steps = 0\n",
    "\n",
    "for run in range(1000):\n",
    "    # reset the environment\n",
    "    obs, _ = env.reset()\n",
    "    currentS = preprocess(obs)\n",
    "\n",
    "    for i in range(0, max_steps):\n",
    "\n",
    "        # hash state\n",
    "        currentS_Key = hashState(currentS)\n",
    "\n",
    "        if currentS_Key not in Q:\n",
    "            Q[currentS_Key] = np.zeros(3)\n",
    "\n",
    "        # Choose an action\n",
    "        a = np.argmax(Q[currentS_Key])\n",
    "\n",
    "        # take action 'a', receive reward 'reward', and observe next state 'obs'\n",
    "        # 'done' indicate if the termination state was reached\n",
    "        obs, reward, done, truncated, info = env.step(a)\n",
    "        \n",
    "        # extract the next state from the observation\n",
    "        nextS = preprocess(obs)\n",
    "\n",
    "        if (done == True):\n",
    "            # if agent reached its goal successfully\n",
    "            mean_reward += reward\n",
    "            completion_rate += 1\n",
    "            average_steps += i+1\n",
    "            break\n",
    "        if (truncated == True):\n",
    "            # agent failed to reach its goal successfully \n",
    "            mean_reward += reward\n",
    "            average_steps += i+1\n",
    "            break\n",
    "\n",
    "        # since the episode is not done, store the next state as the current state for the next step\n",
    "        currentS = nextS\n",
    "    \n",
    "env.close()\n",
    "\n",
    "completion_rate /= 1000\n",
    "average_steps /= 1000\n",
    "print('Completion rate is', completion_rate)\n",
    "print('Average number of steps is', average_steps)\n",
    "print('Average reward over 1000 runs is', np.round(mean_reward/1000, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
